{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from database import connect_to_db_upload\n",
    "import numpy as np\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format CVS\n",
    "def format_csv(csv_path):\n",
    "    # CSV file into a dataframe and format to have datetime and numeric columns\n",
    "    df = pd.read_csv(csv_path, names=['date','counts'])[1:] # Remove extra first row\n",
    "    df[\"date\"] = pd.to_datetime(df['date'])\n",
    "    df['date'] = df['date'].dt.tz_convert('UTC') # convert time zone to UTC\n",
    "    df = df.set_index('date')\n",
    "    df[\"counts\"] = df[\"counts\"].apply(pd.to_numeric)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather(my_station, enddt, startdt):\n",
    "    \"\"\"Main loop.\"\"\"\n",
    "    # print('Entered fetch_weather function')\n",
    "    # Step 1: Fetch global METAR geojson metadata\n",
    "    # https://mesonet.agron.iastate.edu/sites/networks.php\n",
    "    req = requests.get(\n",
    "        \"http://mesonet.agron.iastate.edu/geojson/network/AZOS.geojson\",\n",
    "        timeout=60,\n",
    "    )\n",
    "    geojson = req.json()\n",
    "    for feature in geojson[\"features\"]:\n",
    "        station_id = feature[\"id\"]\n",
    "        if station_id == my_station:\n",
    "            \n",
    "            props = feature[\"properties\"]\n",
    "            # We want stations with data to today (archive_end is null)\n",
    "            if props[\"archive_end\"] is None:\n",
    "                print('archive_end is null = data to today')\n",
    "\n",
    "            # print(f'Fetching data for station {station_id}')\n",
    "            # uri = (\n",
    "            #     \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "            #     f\"station={station_id}&data=all&year1=1928&month1=1&day1=1&\"\n",
    "            #     f\"year2={enddt.year}&month2={enddt.month}&day2={enddt.day}&\"\n",
    "            #     \"tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&\"\n",
    "            #     \"direct=yes&report_type=3\"\n",
    "            # )\n",
    "            uri = (\n",
    "                \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "                f\"station={station_id}&data=all&year1={startdt.year}\"\n",
    "                f\"&month1={startdt.month}&day1={startdt.day}&\"\n",
    "                f\"year2={enddt.year}&month2={enddt.month}&day2={enddt.day}&\"\n",
    "                \"tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&\"\n",
    "                \"direct=yes&report_type=3\"\n",
    "            )\n",
    "            # print('uri: ', uri)\n",
    "\n",
    "            res = requests.get(uri, timeout=300)\n",
    "            # print('received response type: ', type(res))\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_weather_table():\n",
    "\n",
    "    # Home directory\n",
    "    homedir = '../data/'\n",
    "    # Get station ids from detector settings\n",
    "    monitors = pd.read_csv('../detector_info_settings/detector_locations.csv')\n",
    "    ids = monitors['weather_station'].to_list()\n",
    "    ids2 = list(set(monitors['weather_station'].to_list()))\n",
    "    print('ids: ', ids)\n",
    "    \n",
    "    for my_station in ids2:\n",
    "        print('Station id: ', my_station, ' counts: ', ids.count(my_station))\n",
    "        # If several detectors use same station, find the earliest date available\n",
    "        if ids.count(my_station) > 1:\n",
    "            names = monitors.loc[monitors['weather_station'] == my_station, 'name'].to_list()\n",
    "            oldest_dates = []\n",
    "            for detector_name in names:\n",
    "                # If single level folder\n",
    "                try:\n",
    "                    detector_csv = f'{homedir}{detector_name}/{detector_name}_all_logs.csv'\n",
    "                    df = pd.read_csv(detector_csv)\n",
    "                    if 'Unnamed: 0' in df.columns.to_list():\n",
    "                        df = df.rename(columns={'Unnamed: 0':'date'})\n",
    "                    df['date'] = pd.to_datetime(df['date'])\n",
    "                    df = df.set_index('date')\n",
    "\n",
    "                    # get start and end dates of df\n",
    "                    df.sort_index(inplace=True)\n",
    "                    oldest_dates.append(df.head(1).index.values[0])\n",
    "\n",
    "                # Double level folder like Colombo and Serbia\n",
    "                except:\n",
    "                    if 'Det' in detector_name:\n",
    "                        subfolder = detector_name[-4:]\n",
    "                        folder = detector_name[:-5]\n",
    "                    else:\n",
    "                        subfolder = detector_name[-2:]\n",
    "                        folder = detector_name[:-3]\n",
    "                    \n",
    "                    detector_csv = f'{homedir}{folder}/{subfolder}/{detector_name}_all_logs.csv'\n",
    "                    df = pd.read_csv(detector_csv)\n",
    "                    if 'Unnamed: 0' in df.columns.to_list():\n",
    "                        df = df.rename(columns={'Unnamed: 0':'date'})\n",
    "                    df['date'] = pd.to_datetime(df['date'])\n",
    "                    df = df.set_index('date')\n",
    "\n",
    "                    # get start and end dates of df\n",
    "                    df.sort_index(inplace=True)\n",
    "                    oldest_dates.append(df.head(1).index.values[0])\n",
    "            \n",
    "            oldest_df = pd.DataFrame(oldest_dates, columns=['date'])\n",
    "            oldest_df.sort_values(by=['date'], inplace=True)\n",
    "            oldest_ts = oldest_df.head(1)['date'].item()\n",
    "            print('oldest date: ', oldest_ts)\n",
    "\n",
    "\n",
    "        # station for single monitor\n",
    "        else:\n",
    "            detector_name = monitors.loc[monitors['weather_station'] == my_station, 'name'].item()\n",
    "            detector_csv = f'{homedir}{detector_name}/{detector_name}_all_logs.csv'\n",
    "            df = pd.read_csv(detector_csv)\n",
    "            if 'Unnamed: 0' in df.columns.to_list():\n",
    "                df = df.rename(columns={'Unnamed: 0':'date'})\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.set_index('date')\n",
    "\n",
    "            # get start and end dates of df\n",
    "            df.sort_index(inplace=True)\n",
    "            oldest = df.head(1).index.values[0]\n",
    "            oldest_ts = datetime.fromtimestamp(\n",
    "                ((oldest - np.datetime64('1970-01-01T00:00:00'))/ np.timedelta64(1, 's')), \n",
    "                tz=pytz.utc)\n",
    "\n",
    "        # fetch\n",
    "        weatherjson = fetch_weather(my_station, date.today(), oldest_ts)\n",
    "        # Read as cvs from json file format\n",
    "        wdf = pd.read_csv(StringIO(weatherjson.text), sep=',')\n",
    "        wdf[wdf=='M'] = np.nan\n",
    "\n",
    "        # Slice only for needed information based on dates and consider if temperature in farenheit\n",
    "        # print('Columns: ', wdf.columns.to_list())\n",
    "        if 'tmpc' in wdf.columns.to_list() and 'tmpf' not in wdf.columns.to_list():\n",
    "            wdf['tmpc'] = wdf['tmpc'].apply(pd.to_numeric)\n",
    "            wdf['tmpf'] = (wdf['tmpc'] * 9/5) + 32\n",
    "        wdf['tmpf'] = wdf['tmpf'].apply(pd.to_numeric)\n",
    "\n",
    "        if 'mslp' in wdf.columns.to_list():\n",
    "            wdf['mslp'] = wdf['mslp'].apply(pd.to_numeric)\n",
    "        else:\n",
    "            wdf['mslp'] = np.nan\n",
    "        \n",
    "        if 'alti' in wdf.columns.to_list():\n",
    "            wdf['alti'] = wdf['alti'].apply(pd.to_numeric)\n",
    "        else:\n",
    "            wdf['alti'] = np.nan\n",
    "        \n",
    "        # Rename columns\n",
    "        wdf = wdf.rename(columns={'valid':'date', 'tmpf':'temp_in_f', 'mslp':'sea_l_pressure_millibar', 'alti':'alti_pressure'})\n",
    "\n",
    "        # Transform dates and make into index\n",
    "        wdf['date'] = pd.to_datetime(wdf['date'])\n",
    "        wdf = wdf[['date','temp_in_f', 'sea_l_pressure_millibar', 'alti_pressure']]\n",
    "        wdf = wdf.set_index('date')\n",
    "        wdf.sort_index(inplace=True, ascending=True)\n",
    "\n",
    "        # Resample as an hourly df\n",
    "        wdf = wdf.resample('h').mean()\n",
    "\n",
    "        # Remove any 0 values and make into np.nan\n",
    "        wdf.loc[wdf['temp_in_f'] == 0] = np.nan\n",
    "        wdf.loc[wdf['sea_l_pressure_millibar'] == 0] = np.nan\n",
    "        wdf.loc[wdf['alti_pressure'] == 0] = np.nan\n",
    "\n",
    "        # Connect to DB via postgresql and send to db\n",
    "        engine, conn = connect_to_db_upload()\n",
    "        wdf.to_sql(\n",
    "            con=engine, name=f'{my_station.lower()}', if_exists='replace', index_label='date')\n",
    "        print(f'Table {my_station.lower()} sent to DB successfully')\n",
    "\n",
    "        # Make primary key for table via PSYCOPG2\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"\"\"ALTER TABLE {my_station.lower()} ADD PRIMARY KEY (date)\"\"\")\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        print('Query for primary key sent successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids:  ['CQT', 'SKSM', 'DNAA', 'ALM', 'LYBE', 'LYBE', 'VCBI', 'VCBI', 'ATL', 'ATL', 'ATL', 'ATL', 'ATL', 'VCRI']\n",
      "Station id:  VCBI  counts:  2\n",
      "oldest date:  2023-09-15 22:00:00\n",
      "archive_end is null = data to today\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table vcbi sent to DB successfully\n",
      "Query for primary key sent successfully\n"
     ]
    }
   ],
   "source": [
    "download_and_save_weather_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
