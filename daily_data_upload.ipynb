{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from datetime import date, datetime, timedelta\n",
    "from detector_info_settings.detector_format_settings import detector_settings\n",
    "from database import connect_to_db, connect_to_db_upload, format_sql\n",
    "from os import listdir\n",
    "import os\n",
    "from sqlalchemy import text\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting and download functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## format_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name(detector_name_og):\n",
    "    ''' \n",
    "    Creates formatted string for access to detector db info and others\n",
    "\n",
    "    Args:       detector_name_og  -> str\n",
    "    Returns:    str\n",
    "\n",
    "    '''\n",
    "    print('format_name fn')\n",
    "    # Get current data table from db\n",
    "    detector_name = detector_name_og.lower()\n",
    "\n",
    "    # Format detector name to lowercase and check formatting of name \n",
    "    # doesn't have number up front\n",
    "    detector_name = detector_name_og.lower()\n",
    "    if detector_name.startswith('2') or detector_name.startswith('4'):\n",
    "        detector_name = detector_name[1:]+detector_name[0]\n",
    "        \n",
    "    return detector_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_detector_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_data(detector_name):\n",
    "    ''' \n",
    "    Connects to db and downloads data for specified detector\n",
    "\n",
    "    Args:       detector_name       -> str containing detector name for access to settings\n",
    "    Returns:    pandas df\n",
    "\n",
    "    '''\n",
    "    print('get detector data fn')\n",
    "    db = connect_to_db()\n",
    "\n",
    "    with db.connect() as conn:\n",
    "        query = text(f'SELECT * FROM {detector_name}')\n",
    "        result = conn.execute(query)\n",
    "        counts_data = result.fetchall()\n",
    "        conn.close()\n",
    "        db.dispose()\n",
    "\n",
    "        # Format data into pandas df\n",
    "        df = format_sql(counts_data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(station_id):\n",
    "    ''' \n",
    "    Connects to db and downloads data for specified weather station\n",
    "\n",
    "    Args:       station_id      -> str containing detector name for access to settings\n",
    "    Returns:    pandas df\n",
    "\n",
    "    '''\n",
    "    print('get_weather_data fn')\n",
    "    # Connect to db via sqlalchemy\n",
    "    db = connect_to_db()\n",
    "        \n",
    "    with db.connect() as conn:\n",
    "        #Query\n",
    "        query = text(f'SELECT * FROM {station_id}')\n",
    "        result = conn.execute(query)\n",
    "        weather_data = result.fetchall()\n",
    "        conn.close()\n",
    "        db.dispose()\n",
    "        \n",
    "        # Format data\n",
    "        wdf = format_sql(weather_data)\n",
    "\n",
    "        # Format index to allow for querying via datetime format and avoid compatibility issues\n",
    "        if str(wdf.index.tz) != 'UTC':\n",
    "            wdf.index = wdf.index.tz_localize('UTC')\n",
    "    \n",
    "    return wdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather(my_station, enddt, startdt):\n",
    "    ''' \n",
    "    Downloads specified station's data from Iowa State University's website\n",
    "\n",
    "    Args:       my_station      -> str with station id name\n",
    "                enddt           -> str with datetime index for latest known recorded weather data on site\n",
    "                startdt         -> str with datetime index for the last known recorded data on db\n",
    "    Returns:    pandas df\n",
    "\n",
    "    '''\n",
    "\n",
    "    print('fetch_weather fn')\n",
    "    \"\"\"Main loop.\"\"\"\n",
    "    # print('Entered fetch_weather function')\n",
    "    # Step 1: Fetch global METAR geojson metadata\n",
    "    # https://mesonet.agron.iastate.edu/sites/networks.php\n",
    "    req = requests.get(\n",
    "        \"http://mesonet.agron.iastate.edu/geojson/network/AZOS.geojson\",\n",
    "        timeout=60,\n",
    "    )\n",
    "    geojson = req.json()\n",
    "    for feature in geojson[\"features\"]:\n",
    "        station_id = feature[\"id\"]\n",
    "        if station_id == my_station:\n",
    "            \n",
    "            props = feature[\"properties\"]\n",
    "            # We want stations with data to today (archive_end is null)\n",
    "            if props[\"archive_end\"] is None:\n",
    "                print('archive_end is null = data to today')\n",
    "\n",
    "            # print(f'Fetching data for station {station_id}')\n",
    "            # uri = (\n",
    "            #     \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "            #     f\"station={station_id}&data=all&year1=1928&month1=1&day1=1&\"\n",
    "            #     f\"year2={enddt.year}&month2={enddt.month}&day2={enddt.day}&\"\n",
    "            #     \"tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&\"\n",
    "            #     \"direct=yes&report_type=3\"\n",
    "            # )\n",
    "            uri = (\n",
    "                \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "                f\"station={station_id}&data=all&year1={startdt.year}\"\n",
    "                f\"&month1={startdt.month}&day1={startdt.day}&\"\n",
    "                f\"year2={enddt.year}&month2={enddt.month}&day2={enddt.day}&\"\n",
    "                \"tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&\"\n",
    "                \"direct=yes&report_type=3\"\n",
    "            )\n",
    "            # print('uri: ', uri)\n",
    "\n",
    "            res = requests.get(uri, timeout=300)\n",
    "            # print('received response type: ', type(res))\n",
    "            return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily_weather_to_db fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_weather_to_db():\n",
    "    ''' \n",
    "    Downloads specified station's data from Iowa State University's website\n",
    "\n",
    "    Args:       my_station      -> str with station id name\n",
    "                enddt           -> str with datetime index for latest known recorded weather data on site\n",
    "                startdt         -> str with datetime index for the last known recorded data on db\n",
    "    Returns:    pandas df\n",
    "\n",
    "    '''\n",
    "    print('daily_weather_to_db fn')\n",
    "    # Get detector name, path, and station ids from detector settings\n",
    "    detectors = pd.read_csv('detector_info_settings/detector_locations.csv')\n",
    "    station_ids = list(set(detectors['weather_station'].to_list()))\n",
    "    \n",
    "    for my_station in station_ids:\n",
    "\n",
    "        print('Station id: ', my_station, ' counts: ', station_ids.count(my_station))\n",
    "        # Get table from database then sort from most recent to least\n",
    "        weather_db = get_weather_data(my_station)\n",
    "        weather_db = weather_db.resample('h').sum()\n",
    "        weather_db.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "        # Get last known date of data from current table\n",
    "        oldest_ts = pd.to_datetime(weather_db.tail(1).index.values[0])\n",
    "\n",
    "        # fetch\n",
    "        weatherjson = fetch_weather(my_station, date.today(), oldest_ts)\n",
    "        # Read as cvs from json file format\n",
    "        wdf = pd.read_csv(StringIO(weatherjson.text), sep=',')\n",
    "        wdf[wdf=='M'] = np.nan\n",
    "\n",
    "        # Slice only for needed information based on dates and consider if temperature in farenheit\n",
    "        # print('Columns: ', wdf.columns.to_list())\n",
    "        if 'tmpc' in wdf.columns.to_list() and 'tmpf' not in wdf.columns.to_list():\n",
    "            wdf['tmpc'] = wdf['tmpc'].apply(pd.to_numeric)\n",
    "            wdf['tmpf'] = (wdf['tmpc'] * 9/5) + 32\n",
    "        wdf['tmpf'] = wdf['tmpf'].apply(pd.to_numeric)\n",
    "\n",
    "        if 'mslp' in wdf.columns.to_list():\n",
    "            wdf['mslp'] = wdf['mslp'].apply(pd.to_numeric)\n",
    "        else:\n",
    "            wdf['mslp'] = np.nan\n",
    "        \n",
    "        if 'alti' in wdf.columns.to_list():\n",
    "            wdf['alti'] = wdf['alti'].apply(pd.to_numeric)\n",
    "        else:\n",
    "            wdf['alti'] = np.nan\n",
    "        \n",
    "        # Rename columns\n",
    "        wdf = wdf.rename(columns={'valid':'date', 'tmpf':'temp_in_f', 'mslp':'sea_l_pressure_millibar', 'alti':'alti_pressure'})\n",
    "\n",
    "        # Transform dates and make into index\n",
    "        wdf['date'] = pd.to_datetime(wdf['date'], utc=True)\n",
    "        wdf = wdf[['date','temp_in_f', 'sea_l_pressure_millibar', 'alti_pressure']]\n",
    "        wdf = wdf.set_index('date')\n",
    "        wdf.sort_index(inplace=True, ascending=True)\n",
    "\n",
    "        # Merge old and newly downloaded data\n",
    "        merged = pd.concat([weather_db, wdf])\n",
    "        f_merged = merged[~merged.index.duplicated(keep='last')]\n",
    "        f_merged.sort_index(ascending=True, inplace=True)\n",
    "        wdf = f_merged\n",
    "        print('merged: ', wdf.head(1), wdf.tail(1))\n",
    "\n",
    "        # Resample as an hourly df with mean instead of sum\n",
    "        wdf = wdf.resample('h').mean()\n",
    "        \n",
    "        # Remove any 0 values and make into np.nan\n",
    "        wdf.loc[wdf['temp_in_f'] == 0, 'temp_in_f'] = np.nan\n",
    "        wdf.loc[wdf['sea_l_pressure_millibar'] == 0, 'sea_l_pressure_millibar'] = np.nan\n",
    "        wdf.loc[wdf['alti_pressure'] == 0, 'alti_pressure'] = np.nan\n",
    "        \n",
    "        # Remove potential data errors due to shutdowns in some areas\n",
    "        # Caused by power outages like Abuja\n",
    "        wdf.loc[\n",
    "            (wdf['temp_in_f'] > wdf['temp_in_f'].mean() + (4*wdf['temp_in_f'].std())) |\n",
    "            (wdf['temp_in_f'] < wdf['temp_in_f'].mean() - (4*wdf['temp_in_f'].std()))] = np.nan\n",
    "        wdf.loc[\n",
    "            (wdf['sea_l_pressure_millibar'] > wdf['sea_l_pressure_millibar'].mean() + (4*wdf['sea_l_pressure_millibar'].std())) |\n",
    "            (wdf['sea_l_pressure_millibar'] < wdf['sea_l_pressure_millibar'].mean() - (4*wdf['sea_l_pressure_millibar'].std()))] = np.nan\n",
    "        wdf.loc[\n",
    "            (wdf['alti_pressure'] > wdf['alti_pressure'].mean() + (4*wdf['alti_pressure'].std())) |\n",
    "            (wdf['alti_pressure'] < wdf['alti_pressure'].mean() - (4*wdf['alti_pressure'].std()))] = np.nan\n",
    "\n",
    "        # Connect to DB via postgresql and send to db\n",
    "        engine, conn = connect_to_db_upload()\n",
    "        wdf.to_sql(\n",
    "            con=engine, name=f'{my_station.lower()}', if_exists='replace', index_label='date')\n",
    "        print(f'Table {my_station.lower()} sent to DB successfully')\n",
    "\n",
    "        # Make primary key for table via PSYCOPG2\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"\"\"ALTER TABLE {my_station.lower()} ADD PRIMARY KEY (date)\"\"\")\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        print('Query for primary key sent successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Data processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_adding_all_timestamps_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_adding_all_timestamps_hourly(merged_logs, log_df, lfreq, trim_base_df, trim_log_df):\n",
    "    ''' \n",
    "    Merges two provided log df files, first being the base or earliest data, second, most recent data,\n",
    "    then trims for error reduction and adjusts data frequency to hourly\n",
    "\n",
    "    Args:       detector_name_path  -> folder containing log files to be merged\n",
    "                detector_name       -> str containing detector name for access to settings\n",
    "    Returns:    merged pandas df\n",
    "\n",
    "    '''\n",
    "    print('merge_adding_all_timestamps_hourly fn')\n",
    "    # Format df to hourly and trim if requested\n",
    "    if not merged_logs.empty and trim_base_df:\n",
    "        temp = merged_logs.resample('h').sum()\n",
    "        # Basic trimming of first and last hour due to timestamps missing\n",
    "        temp.drop(temp.head(1).index, inplace=True)\n",
    "        temp.drop(temp.tail(1).index, inplace=True)\n",
    "        # Replace 0 values to nan\n",
    "        temp.loc[temp['counts'] == 0] = np.nan\n",
    "        # If initial values on df are all nan, remove until valid value found\n",
    "        temp = temp[temp.first_valid_index():]\n",
    "        merged_logs = temp\n",
    "\n",
    "    if not log_df.empty and trim_log_df:\n",
    "        temp = log_df.resample('h').sum()\n",
    "        temp.drop(temp.head(1).index, inplace=True)\n",
    "        temp.drop(temp.tail(1).index, inplace=True)\n",
    "        temp.loc[temp['counts'] == 0] = np.nan\n",
    "        temp = temp[temp.first_valid_index():]\n",
    "        log_df = temp\n",
    "\n",
    "    # Check if df is not empty after trimming and perform merge\n",
    "    if not merged_logs.empty and not log_df.empty:\n",
    "\n",
    "        # Extract first date from log df to be merged at end of current base df\n",
    "        first_log_date = str(log_df.head(1).index[0])\n",
    "        # Extract last date from base df\n",
    "        last_log_date = str(merged_logs.tail(1).index[0])\n",
    "\n",
    "        # Check if dates are equal or in the right chronological order ascending\n",
    "        if str(merged_logs.head(1).index[0]) <= first_log_date:\n",
    "            # If able to find first log date from log df on merged_logs, no detector \n",
    "            # has shutdown and just need to add at end of file whatever is not yet part of base df\n",
    "            try:\n",
    "                bf_first_log_date = str(merged_logs.iloc[merged_logs.get_loc(first_log_date) - 1].name)\n",
    "                print('No shutdown since last log data integration')\n",
    "                # Merge\n",
    "                merged = pd.concat([merged_logs, log_df])\n",
    "                final_merged_logs = merged[~merged.index.duplicated(keep='last')]\n",
    "                # Sort fixed merged log files\n",
    "                merged = final_merged_logs.sort_index()\n",
    "            \n",
    "            # else, logs must be merged considering missing counts due to detector shutoff time\n",
    "            except:\n",
    "                print('Last log date bf shutdown: ', last_log_date, '\\nFirst log date after shutdown: ', first_log_date)\n",
    "                # Create continuous range of dates to complete df timeline for missing date indexes\n",
    "                date_range = pd.date_range(start=last_log_date, end=first_log_date, freq=lfreq, tz='UTC')\n",
    "                # Create df containing missing dates as index and np.nan values\n",
    "                offline_df = pd.DataFrame(index=date_range, columns=merged_logs.columns, data=np.nan)\n",
    "                # Update current logs to include missing detector data time stamps but with nan values for graph\n",
    "                complete_logs = pd.concat([offline_df, log_df])\n",
    "                merged = pd.concat([merged_logs, complete_logs])\n",
    "                final_merged_logs = merged[~merged.index.duplicated(keep='last')]\n",
    "                # Sort fixed merged log files\n",
    "                merged = final_merged_logs.sort_index()\n",
    "\n",
    "        # Reverse which file is merged at end of the other to ensure chronological order\n",
    "        else:\n",
    "            # Extract first date from log df to be merged at end of current base df\n",
    "            first_log_date = str(merged_logs.head(1).index[0])\n",
    "            # Extract last date from base df\n",
    "            last_log_date = str(log_df.tail(1).index[0])\n",
    "\n",
    "            try:\n",
    "                bf_first_log_date = str(log_df.iloc[log_df.get_loc(first_log_date) - 1].name)\n",
    "                print('No shutdown since last log data integration')\n",
    "                # Merge\n",
    "                merged = pd.concat([log_df, merged_logs])\n",
    "                final_merged_logs = merged[~merged.index.duplicated(keep='last')]\n",
    "                # Sort fixed merged log files\n",
    "                merged = final_merged_logs.sort_index()\n",
    "            \n",
    "            # else, logs must be merged considering missing counts due to detector shutoff time\n",
    "            except:\n",
    "                print('Last log date bf shutdown: ', last_log_date, '\\nFirst log date after shutdown: ', first_log_date)\n",
    "                # Create continuous range of dates to complete df timeline for missing date indexes\n",
    "                date_range = pd.date_range(start=last_log_date, end=first_log_date, freq=lfreq, tz='UTC')\n",
    "                # Create df containing missing dates as index and np.nan values\n",
    "                offline_df = pd.DataFrame(index=date_range, columns=merged_logs.columns, data=np.nan)\n",
    "                # Update current logs to include missing detector data time stamps but with nan values for graph\n",
    "                complete_logs = pd.concat([offline_df, merged_logs])\n",
    "                merged = pd.concat([log_df, complete_logs])\n",
    "                final_merged_logs = merged[~merged.index.duplicated(keep='last')]\n",
    "                # Sort fixed merged log files\n",
    "                merged = final_merged_logs.sort_index()\n",
    "    \n",
    "    # Only merged logs remains after trimming\n",
    "    elif not merged_logs.empty:\n",
    "        merged = merged_logs\n",
    "    # Only log df remains after trimming\n",
    "    elif not log_df.empty:\n",
    "        merged = log_df\n",
    "    # None have data, return empty df\n",
    "    else:\n",
    "        merged = pd.DataFrame()\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_logs_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_log_dfs(log_dfs_list, detector_name):\n",
    "    '''  \n",
    "    Organizes each log df within a list of dfs chronologically, then merges them all.\n",
    "    This includes adding missing time between end of one log to the other as np.NaN\n",
    "    values due to monitor shutdowns.\n",
    "\n",
    "    Args:       detector_name -> string containing detector name from settings\n",
    "                log_dfs_list -> list containing n number of log dfs from a specific\n",
    "                                monitor data folder\n",
    "    Returns:    merged pandas df with all logs\n",
    "\n",
    "    '''\n",
    "    print('merge_log_dfs fn')\n",
    "    if len(log_dfs_list) == 0 or log_dfs_list == None:\n",
    "        print('Given list of log dfs is empty. Try again with to run the all_monitor_logs_to_dfs() with correct file path')\n",
    "    elif len(log_dfs_list) > 1:\n",
    "        \n",
    "        # Sort logs from latest to earliest based on initial log date to ensure optimal merging\n",
    "        # Extract first row date of each log df into list\n",
    "        logs_l = [str(df.head(1).index[0]) for df in log_dfs_list]\n",
    "        # Make dictionary containing index of corresponding date on provided logs list\n",
    "        logs_dict = {}\n",
    "        for i in range(len(logs_l)):\n",
    "            logs_dict[logs_l[i]] = i\n",
    "        # sort values in ascending order\n",
    "        logs_l.sort()\n",
    "        # Create sorted list of logs\n",
    "        new_logs_df_list = []\n",
    "        for i in range(len(logs_l)):\n",
    "            new_logs_df_list.append(log_dfs_list[logs_dict[logs_l[i]]])\n",
    "        \n",
    "        # Extract first df on list and to start merging with rest of dfs\n",
    "        merged_logs = new_logs_df_list[0].copy(deep=True)\n",
    "        # Extract frequency of data from given monitor\n",
    "        lfreq = detector_settings[detector_name]['freq']\n",
    "        # Flag for initial merged df to be trimmed for incomplete hourly logs\n",
    "        trim_initial_merged = True\n",
    "\n",
    "        # Navigate through list of dfs and merge, making the df hourly\n",
    "        for i in range(1, len(new_logs_df_list)):\n",
    "            print(f'Log file index {i}')\n",
    "            merged_logs = merge_adding_all_timestamps_hourly(merged_logs, new_logs_df_list[i], lfreq, trim_initial_merged, True)\n",
    "            trim_initial_merged = False # Set flag to false to no longer trim base df as it has been done\n",
    "\n",
    "    # Else, the df list only has one df so no merging within list\n",
    "    else: \n",
    "        merged_logs = new_logs_df_list[0].copy(deep=True)\n",
    "        # Sample by hour\n",
    "        merged_logs1 = merged_logs.resample('h').sum()\n",
    "        # Trim first and last hour because of data missing due to log being incomplete\n",
    "        merged_logs1.drop(merged_logs1.head(1).index, inplace=True)\n",
    "        merged_logs1.drop(merged_logs1.tail(1).index, inplace=True)\n",
    "        merged_logs1.loc[merged_logs1['counts'] == 0] = np.nan\n",
    "        \n",
    "        # Reassign for return statement accuracy\n",
    "        merged_logs = merged_logs1\n",
    "\n",
    "    return merged_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_df_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_df_formatting(logs_df, detector_name):\n",
    "    ''' \n",
    "    Formats given logs into a datetime and counts column df. If desired, you can keep the\n",
    "    additional counts columns in case needed for later.\n",
    "    \n",
    "    Args:       logs_df         -> pandas df containing non-formatted data where only one column\n",
    "                                    named 'counts' exists and has all data.\n",
    "                detector_name    -> indicates how to split the row based on specific detector's\n",
    "                                    row content, since some have more data than others\n",
    "    \n",
    "    Returns:    pandas df containing formatted logs as date index and counts column\n",
    "\n",
    "    '''\n",
    "    print('log_df_formatting fn')\n",
    "    # Get specific split number of elements depending monitor log file data format\n",
    "    splits = detector_settings[detector_name]['splits']\n",
    "    # Get counts column\n",
    "    counts_col = detector_settings[detector_name]['counts_col']\n",
    "    # Get index for date column\n",
    "    date_col = detector_settings[detector_name]['date_col']\n",
    "    # Get corresponding timezone\n",
    "    monitor_tz = detector_settings[detector_name]['timezone']\n",
    "\n",
    "    # Make copy to ensure it is not damaging current logs data\n",
    "    temp = logs_df.copy(deep=True)\n",
    "\n",
    "    # Format df to remove tracing commas after counts values if the separation is based on spacing\n",
    "    if temp['counts'].str.contains(' ').any() & temp['counts'].str.contains(',').any():\n",
    "        temp['counts'] = temp['counts'].str.replace(',','', regex=True)\n",
    "        \n",
    "    # Split counts column into n elements (i.e.: if n=4, containing count1, count2, count3, day)\n",
    "    # separation is space-based \" \"\n",
    "    if temp['counts'].str.contains(' ').any():\n",
    "        temp = temp['counts'].str.split(\" \", n = splits, expand=True)\n",
    "    # else, separation is comma-based \",\"\n",
    "    else:\n",
    "        temp = temp['counts'].str.split(\",\", n=splits, expand=True)\n",
    "    \n",
    "    # Slice df to only have desired columns counts and date\n",
    "    temp = temp.rename(columns={date_col:'date',counts_col:'counts'})\n",
    "    ldf = temp[['date', 'counts']]\n",
    "\n",
    "    # transform date column string into a datetime format type\n",
    "    ldf['date'] = pd.to_datetime(ldf['date'], format='mixed')\n",
    "    # Localize to monitor location's timezone, ambiguous to infer fall DST change an hour back, \n",
    "    # and nonexistent for clocks moving forward due to DST\n",
    "    try:\n",
    "        # It infers the date and shifts forward if dailight savings causes errors\n",
    "        ldf['date'] = ldf['date'].dt.tz_localize(monitor_tz, ambiguous='infer', nonexistent='shift_forward')\n",
    "    except:\n",
    "        try:\n",
    "            # If duplicate dates due to localization into a timezone with daylight, choose to keep one\n",
    "            ldf['date'] = ldf['date'].dt.tz_localize(monitor_tz, ambiguous=True, nonexistent='shift_forward')\n",
    "        except:\n",
    "            # Else, the df is already localized to a particular timezone\n",
    "            print('DF date already timezone aware so no need!')\n",
    "\n",
    "    # Format to UTC so it can be merged with the weather app's data that is also UTC\n",
    "    ldf['date'] = ldf['date'].dt.tz_convert('UTC')\n",
    "    # Set date column as index\n",
    "    ldf = ldf.set_index('date')\n",
    "    # Make counts column into numeric type\n",
    "    ldf['counts'] = ldf['counts'].apply(pd.to_numeric)\n",
    "\n",
    "    # Remove any errors due to NaT values on index date column\n",
    "    try:\n",
    "        ldf2 = ldf.drop('NaT')\n",
    "    except:\n",
    "        ldf2 = ldf\n",
    "\n",
    "    # Sort df based on date index\n",
    "    ldf2.sort_index()\n",
    "    \n",
    "    return ldf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_detector_logs_to_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_detector_logs_to_dfs(detector_name_path, detector_name):\n",
    "    ''' \n",
    "    Creates a df of merged log files. This assumes all logs within given monitor folder\n",
    "    name share same format style. Log files should be reviewed before applying this function.\n",
    "\n",
    "    Args:       detector_name_path  -> folder containing log files to be merged\n",
    "                detector_name       -> str containing detector name for access to settings\n",
    "    Returns:    merged pandas df\n",
    "\n",
    "    '''\n",
    "    print('all_detector_logs_to_dfs fn')\n",
    "    # list of files within selected monitor\n",
    "    all_files = listdir(detector_name_path)\n",
    "    # filter based on ending being .log\n",
    "    logs_list = [f for f in all_files if f.endswith('.log')]\n",
    "\n",
    "    # df = pd.DataFrame()\n",
    "    logs_df_list = []\n",
    "\n",
    "    for log_file in logs_list:\n",
    "        # Read log file as df separating rows\n",
    "        temp = pd.read_csv(detector_name_path+'/'+log_file, sep='\\t', names=['counts'])\n",
    "        # If df has column titles, remove as it causes issues with below functions\n",
    "        if temp.head(1)['counts'].str.contains('date').any():\n",
    "            temp.drop(temp.head(1).index, inplace=True)\n",
    "        # Format df based on monitor settings\n",
    "        temp = log_df_formatting(temp, detector_name)\n",
    "        # Append to list of dfs\n",
    "        logs_df_list.append(temp)\n",
    "\n",
    "    return logs_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce_shutdown_count_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_shutdown_count_errors(df):\n",
    "    ''' \n",
    "    Removes any values before and after shutdowns (up to 3) where values are greater or less than\n",
    "    the mean for a 24 (or less, if not enough available) window of data +- the standard dev.*1.5\n",
    "\n",
    "    Args:       df\n",
    "    Returns:    df\n",
    "\n",
    "    '''\n",
    "    print('reduce_shutdown_count_errors fn')\n",
    "\n",
    "    # Ensure df is hourly\n",
    "    df = df.resample('h').sum()\n",
    "    # Re-establish np.nan values due to above function replacing them with 0s\n",
    "    df.loc[df['counts'] == 0] = np.nan\n",
    "\n",
    "    # Add missing dates to ensure graph shows correct on website (i.e. offline is demarket instead of assuming online)\n",
    "    start = pd.to_datetime(df.head(1).index.values[0], utc=True)\n",
    "    end = pd.to_datetime(df.tail(1).index.values[0], utc=True)\n",
    "    t = pd.date_range(start=start, end=end, freq='h', tz='UTC')\n",
    "    tdf = pd.DataFrame(index=t, columns=df.columns, data=np.nan)\n",
    "    df1 = pd.concat([tdf, df])\n",
    "    df2 = df1[~df1.index.duplicated(keep='last')]\n",
    "    df = df2.sort_index(ascending=True)\n",
    "\n",
    "    # Do initial cleanup of anything above or below the mean + std*3\n",
    "    df.loc[(df['counts'] > df['counts'].mean() + (3*df['counts'].std())) | (df['counts'] < df['counts'].mean() - (3*df['counts'].std()))] = np.nan\n",
    "    df = df[(df.first_valid_index()):df.last_valid_index()]\n",
    "    \n",
    "    # Extract initial indexes if any\n",
    "    f = df.index.get_loc(df.first_valid_index())\n",
    "    l = df.index.get_loc(pd.to_datetime(df.isna().idxmax().head(1).values[0], utc=True))\n",
    "\n",
    "    # If the last index for interval is the same as initial index of df, it means\n",
    "    # there have never been any disruptions on data due to shutdowns\n",
    "    if l != f:\n",
    "\n",
    "        while l <= len(df) and f != l:\n",
    "            # Deal with spikes after shutdowns, calculate mean and std\n",
    "            if (f+24) < l:\n",
    "                mean = df[f:(f+24)].mean().values[0]\n",
    "                std = df[f:(f+24)].std().values[0]\n",
    "            else:\n",
    "                mean = df[f:(l - 1)].mean().values[0]\n",
    "                std = df[f:(l - 1)].std().values[0]\n",
    "            \n",
    "            # Check if first three values are lesser or greater than mean +- std, then assign np.nan\n",
    "            if df.iloc[f]['counts'] < (mean - (std*1.5)) or df.iloc[f]['counts'] > (mean + (std*2)):\n",
    "                df.iloc[f]['counts'] = np.nan\n",
    "            if f+1 < len(df) and df.iloc[f+1]['counts'] < (mean - (std*1.5)) or df.iloc[f+2]['counts'] > (mean + (std*2)):\n",
    "                df.iloc[(f+1)]['counts'] = np.nan\n",
    "            if f+2 < len(df) and df.iloc[f+2]['counts'] < (mean - (std*1.5)) or df.iloc[f+2]['counts'] > (mean + (std*2)):\n",
    "                df.iloc[(f+2)]['counts'] = np.nan\n",
    "\n",
    "            # Deal with spikes before shutdowns\n",
    "            if (l-25) > f:\n",
    "                mean = df[(l - 25):(l - 1)].mean().values[0]\n",
    "                std = df[(l - 25):(l - 1)].std().values[0]\n",
    "            else:\n",
    "                mean = df[f:(l - 1)].mean().values[0]\n",
    "                std = df[f:(l - 1)].std().values[0]\n",
    "\n",
    "            # Check if last three values are lesser or greater than mean +- std, then assign np.nan\n",
    "            if df.iloc[l-1]['counts'] < (mean - (std*1.5)) or df.iloc[l-1]['counts'] > (mean + (std*2)):\n",
    "                df.iloc[(l-1)]['counts'] = np.nan\n",
    "            if  l-2 > 0 and df.iloc[l-2]['counts'] < (mean - (std*1.5)) or df.iloc[l-2]['counts'] > (mean + (std*2)):\n",
    "                df.iloc[(l-2)]['counts'] = np.nan\n",
    "            if l-3 > 0 and df.iloc[l-3]['counts'] < (mean - (std*1.5)) or df.iloc[l-3]['counts'] > (mean + (std*2)):\n",
    "                df.iloc[(l-3)]['counts'] = np.nan\n",
    "\n",
    "            # Obtain new start and end index for evaluation of next df slice\n",
    "            f = df.index.get_loc(df[l:].first_valid_index())\n",
    "            l = df.index.get_loc(pd.to_datetime(df[f:].isna().idxmax().head(1).values[0], utc=True))\n",
    "            \n",
    "            # if l == f:\n",
    "            #     break\n",
    "\n",
    "    # Ensure all values == 0 are not accounted numerically during graphing and mean/std calculations\n",
    "    df.loc[df['counts'] == 0] = np.nan\n",
    "    \n",
    "    df = df.sort_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_and_upload_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_upload_logs(detector_data, detector_file_path, detector_name_og, detector_name):\n",
    "    ''' \n",
    "    Function that handles the overall processing of new logs and insertion into table\n",
    "\n",
    "    Args:       detector_data       -> df containing all detector logs\n",
    "                detector_file_path  -> local/server path for files for specified detector locations\n",
    "                detector_name_og    -> name as str without any formatting enforced for naming files and subfolder\n",
    "                detector_name       -> name as str having been formatted for purposes of accessing right table on \n",
    "                                        db\n",
    "\n",
    "    '''\n",
    "    print('process_and_upload_logs fn')\n",
    "    # List of formatted logs into dfs\n",
    "    log_dfs_list = all_detector_logs_to_dfs(detector_file_path, detector_name_og)\n",
    "\n",
    "    # Merge all found logs into a df\n",
    "    hourly_logs = merge_log_dfs(log_dfs_list, detector_name_og)\n",
    "\n",
    "    # Merge db data with new log data\n",
    "    df = pd.concat([detector_data, hourly_logs])\n",
    "    df1 = df[~df.index.duplicated(keep='last')]\n",
    "    df1 = df1.sort_index(ascending=True)\n",
    "    \n",
    "    # Filter out low counts or out of standard deviation data each time detectors disconnect\n",
    "    df = reduce_shutdown_count_errors(df1)\n",
    "    \n",
    "    # Upload to db\n",
    "    engine, conn = connect_to_db_upload()\n",
    "    cur = conn.cursor()\n",
    "    df.to_sql(con=engine, name=f'{detector_name}', if_exists='replace', index_label='date')\n",
    "    print('Table sent to DB successfully')\n",
    "    \n",
    "    # Make primary key for table via PSYCOPG2\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"ALTER TABLE {detector_name} ADD PRIMARY KEY (date)\"\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    print('Query for primary key sent successfully')\n",
    "\n",
    "    # Delete log files to avoid clutter since already on db\n",
    "    l=glob.glob(os.path.join(detector_file_path, '*.log'))\n",
    "\n",
    "    if len(l) <=1:\n",
    "        print('No files or only one file found.')\n",
    "    else:\n",
    "        l.sort(key=os.path.getmtime, reverse=True)\n",
    "        print(l)\n",
    "\n",
    "        del_l = l[1:]\n",
    "\n",
    "        for file in del_l:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f'deleting {file}')\n",
    "            except:\n",
    "                print(f'Error deleting file/file not found - {file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily_logs_to_db fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_logs_to_db():\n",
    "    print('daily_logs_and_weather_to_db fn')\n",
    "    # Home directory\n",
    "    homedir = 'data/'\n",
    "\n",
    "    # Get detector name, path, and station ids from detector settings\n",
    "    detectors = pd.read_csv('detector_info_settings/detector_locations.csv')\n",
    "    detectors_info = detectors[['name','name_path', 'weather_station']].values.tolist()\n",
    "    \n",
    "\n",
    "    # For each available detector row within settings csv as df\n",
    "    for row in detectors_info:\n",
    "        print('\\n\\n**************\\nDetector: ', row[0])\n",
    "        # Format name for file naming and db table access\n",
    "        detector_name = format_name(detector_name_og=row[0])\n",
    "        \n",
    "        # Download db tables as dfs\n",
    "        detector_db = get_detector_data(detector_name=detector_name)\n",
    "        \n",
    "        # Process detector count logs, join them to db table and upload to db, then delete older files\n",
    "        process_and_upload_logs(detector_data=detector_db, detector_file_path=f'{homedir}/{row[1]}', detector_name_og=row[0], detector_name=detector_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute daily_logs_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_logs_and_weather_to_db fn\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Chara_Muon002\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 15:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-09.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-08.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-07.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-06.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-05.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-04.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-03.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-02.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-01.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-08-31.log', 'data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-08-25.log']\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-08.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-07.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-06.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-05.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-04.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-03.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-02.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-09-01.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-08-31.log\n",
      "deleting data//Chara_Muon002/Chara_Muon002_2024-07-21_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  SantaMarta\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-31 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-22 22:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-09.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-08.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-07.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-06.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-05.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-04.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-03.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-02.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-01.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-08-31.log', 'data//SantaMarta/SantaMarta_2024_08_22_to_2024-08-25.log']\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-08.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-07.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-06.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-05.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-04.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-03.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-02.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-09-01.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-08-31.log\n",
      "deleting data//SantaMarta/SantaMarta_2024_08_22_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Abuja\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-03 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 10:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-03 21:00:00+00:00 \n",
      "First log date after shutdown:  2024-09-04 07:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-04 23:00:00+00:00 \n",
      "First log date after shutdown:  2024-09-04 07:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-04 23:00:00+00:00 \n",
      "First log date after shutdown:  2024-09-04 07:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-04 23:00:00+00:00 \n",
      "First log date after shutdown:  2024-09-04 07:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-04 23:00:00+00:00 \n",
      "First log date after shutdown:  2024-09-04 07:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Abuja/Abuja_2024_09_04_to_2024-09-09.log', 'data//Abuja/Abuja_2024_09_04_to_2024-09-08.log', 'data//Abuja/Abuja_2024_09_04_to_2024-09-07.log', 'data//Abuja/Abuja_2024_09_04_to_2024-09-06.log', 'data//Abuja/Abuja_2024_09_04_to_2024-09-05.log', 'data//Abuja/Abuja_2024_08_30_to_2024-09-04.log', 'data//Abuja/Abuja_2024_08_30_to_2024-09-03.log', 'data//Abuja/Abuja_2024_08_29_to_2024-09-02.log', 'data//Abuja/Abuja_2024_08_29_to_2024-09-01.log', 'data//Abuja/Abuja_2024_08_29_to_2024-08-31.log', 'data//Abuja/Abuja_2024_08_23_to_2024-08-25.log']\n",
      "deleting data//Abuja/Abuja_2024_09_04_to_2024-09-08.log\n",
      "deleting data//Abuja/Abuja_2024_09_04_to_2024-09-07.log\n",
      "deleting data//Abuja/Abuja_2024_09_04_to_2024-09-06.log\n",
      "deleting data//Abuja/Abuja_2024_09_04_to_2024-09-05.log\n",
      "deleting data//Abuja/Abuja_2024_08_30_to_2024-09-04.log\n",
      "deleting data//Abuja/Abuja_2024_08_30_to_2024-09-03.log\n",
      "deleting data//Abuja/Abuja_2024_08_29_to_2024-09-02.log\n",
      "deleting data//Abuja/Abuja_2024_08_29_to_2024-09-01.log\n",
      "deleting data//Abuja/Abuja_2024_08_29_to_2024-08-31.log\n",
      "deleting data//Abuja/Abuja_2024_08_23_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  APO\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-02-26 22:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//APO/APO_2024_02_26_to_2024-09-09.log', 'data//APO/APO_2024_02_26_to_2024-09-08.log', 'data//APO/APO_2024_02_26_to_2024-09-07.log', 'data//APO/APO_2024_02_26_to_2024-09-06.log', 'data//APO/APO_2024_02_26_to_2024-09-05.log', 'data//APO/APO_2024_02_26_to_2024-09-04.log', 'data//APO/APO_2024_02_26_to_2024-09-03.log', 'data//APO/APO_2024_02_26_to_2024-09-02.log', 'data//APO/APO_2024_02_26_to_2024-09-01.log', 'data//APO/APO_2024_02_26_to_2024-08-31.log', 'data//APO/APO_2024_02_26_to_2024-08-25.log']\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-08.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-07.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-06.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-05.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-04.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-03.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-02.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-09-01.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-08-31.log\n",
      "deleting data//APO/APO_2024_02_26_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Serbia_Belgrade_Det1\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-09.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-08.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-07.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-06.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-05.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-04.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-03.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-02.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-01.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-08-31.log', 'data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-08-25.log']\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-08.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-07.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-06.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-05.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-04.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-03.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-02.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-09-01.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-08-31.log\n",
      "deleting data//Serbia_Belgrade/Det1/Serbia_Belgrade_Det1_2024_07_22_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Serbia_Belgrade_Det2\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-19 08:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-22 14:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-09.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-08.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-07.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-06.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-05.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-04.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-03.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-02.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-01.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-08-31.log', 'data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-08-25.log']\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-08.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-07.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-06.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-05.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-04.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-03.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-02.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-09-01.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-08-31.log\n",
      "deleting data//Serbia_Belgrade/Det2/Serbia_Belgrade_Det2_2024_07_22_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Colombo_V1\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-07-15 20:00:00+00:00 \n",
      "First log date after shutdown:  2024-06-05 11:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-09.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-08.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-07.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-06.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-05.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-04.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-03.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-02.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-01.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-08-31.log', 'data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-08-25.log']\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-08.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-07.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-06.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-05.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-04.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-03.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-02.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-09-01.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-08-31.log\n",
      "deleting data//Colombo/V1/Colombo_V1_2024-06-05_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Colombo_V2\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 17:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-20 18:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-09.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-08.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-07.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-06.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-05.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-04.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-03.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-02.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-01.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-08-31.log', 'data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-08-25.log']\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-08.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-07.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-06.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-05.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-04.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-03.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-02.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-09-01.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-08-31.log\n",
      "deleting data//Colombo/V2/Colombo_V2_2024_08_20_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  2Paddle\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-11 23:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//2Paddle/2Paddle_2024-07-11_to_2024-09-09.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-08.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-07.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-06.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-05.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-04.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-03.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-02.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-09-01.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-08-31.log', 'data//2Paddle/2Paddle_2024-07-11_to_2024-08-25.log']\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-08.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-07.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-06.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-05.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-04.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-03.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-02.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-09-01.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-08-31.log\n",
      "deleting data//2Paddle/2Paddle_2024-07-11_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  4Paddle\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-07 18:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//4Paddle/4Paddle_2024-08-07_to_2024-09-09.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-08.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-07.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-06.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-05.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-04.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-03.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-02.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-09-01.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-08-31.log', 'data//4Paddle/4Paddle_2024-08-07_to_2024-08-25.log']\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-08.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-07.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-06.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-05.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-04.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-03.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-02.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-09-01.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-08-31.log\n",
      "deleting data//4Paddle/4Paddle_2024-08-07_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  MarkV\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2023-02-18 11:00:00+00:00 \n",
      "First log date after shutdown:  2022-08-08 17:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//MarkV/MarkV_2022-08-08_to_2024-09-09.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-08.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-07.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-06.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-05.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-04.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-03.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-02.log', 'data//MarkV/MarkV_2022-08-08_to_2024-09-01.log', 'data//MarkV/MarkV_2022-08-08_to_2024-08-31.log', 'data//MarkV/MarkV_2022-08-08_to_2024-08-25.log']\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-08.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-07.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-06.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-05.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-04.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-03.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-02.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-09-01.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-08-31.log\n",
      "deleting data//MarkV/MarkV_2022-08-08_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  OneParkPlace\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-07 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-15 20:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-09.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-08.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-07.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-06.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-05.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-04.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-03.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-02.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-01.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-08-31.log', 'data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-08-25.log']\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-08.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-07.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-06.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-05.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-04.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-03.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-02.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-09-01.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-08-31.log\n",
      "deleting data//OneParkPlace/OneParkPlace_2024_07_15_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  Rm415_Muon001\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "log_df_formatting fn\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-08-25 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-08 02:00:00+00:00 \n",
      "First log date after shutdown:  2024-08-30 18:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-09.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-08.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-07.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-06.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-05.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-04.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-03.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-02.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-01.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-08-31.log', 'data//Rm415_Muon001/Rm415_Muon001_2024_08_23_to_2024-08-25.log']\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-08.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-07.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-06.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-05.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-04.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-03.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-02.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-09-01.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_30_to_2024-08-31.log\n",
      "deleting data//Rm415_Muon001/Rm415_Muon001_2024_08_23_to_2024-08-25.log\n",
      "\n",
      "\n",
      "**************\n",
      "Detector:  UvaWellassa_Muon001\n",
      "format_name fn\n",
      "get detector data fn\n",
      "Connected to muon database successfully\n",
      "UTC\n",
      "process_and_upload_logs fn\n",
      "all_detector_logs_to_dfs fn\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "log_df_formatting fn\n",
      "DF date already timezone aware so no need!\n",
      "merge_log_dfs fn\n",
      "Log file index 1\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 2\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 3\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 4\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 5\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 6\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 7\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 8\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 9\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "Log file index 10\n",
      "merge_adding_all_timestamps_hourly fn\n",
      "Last log date bf shutdown:  2024-09-09 03:00:00+00:00 \n",
      "First log date after shutdown:  2024-07-21 06:00:00+00:00\n",
      "reduce_shutdown_count_errors fn\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "['data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-09.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-08.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-07.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-06.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-05.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-04.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-03.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-02.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-01.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-08-31.log', 'data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-08-25.log']\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-08.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-07.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-06.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-05.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-04.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-03.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-02.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-09-01.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-08-31.log\n",
      "deleting data//UvaWellassa_Muon001/UvaWellassa_Muon001_2024-07-21_to_2024-08-25.log\n"
     ]
    }
   ],
   "source": [
    "daily_logs_to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute daily_weather_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_weather_to_db fn\n",
      "Station id:  VCBI  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-09-15 00:00:00+00:00       77.0                      0.0          29.83                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:10:00+00:00       82.4                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-09-15 00:00:00+00:00       77.0                      0.0          29.83                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       82.4                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-09-15 00:00:00+00:00       77.0                      NaN          29.83                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       82.4                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-09-15 00:00:00+00:00       77.0                      NaN          29.83                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       82.4                      NaN           29.8\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table vcbi sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  DNAA  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-01 15:00:00+00:00       87.8                      0.0          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       75.2                      NaN          29.97\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-01 15:00:00+00:00       87.8                      0.0          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       75.2                      NaN          29.97\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-01 15:00:00+00:00       87.8                      NaN          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       75.2                      NaN          29.97\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-01 15:00:00+00:00       87.8                      NaN          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       75.2                      NaN          29.97\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table dnaa sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  LYBE  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-06-17 12:00:00+00:00       84.2                      0.0           30.0                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       48.2                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-06-17 12:00:00+00:00       84.2                      0.0           30.0                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       48.2                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-06-17 12:00:00+00:00       84.2                      NaN           30.0                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       48.2                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-06-17 12:00:00+00:00       84.2                      NaN           30.0                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       48.2                      NaN           29.8\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table lybe sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  SKSM  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-08-15 23:00:00+00:00       89.6                      0.0          29.77                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       86.0                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-08-15 23:00:00+00:00       89.6                      0.0          29.77                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       86.0                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-08-15 23:00:00+00:00       89.6                      NaN          29.77                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       86.0                      NaN           29.8\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-08-15 23:00:00+00:00       89.6                      NaN          29.77                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       86.0                      NaN           29.8\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table sksm sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  VCRI  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-06-16 06:00:00+00:00       93.2                      0.0          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 21:10:00+00:00       78.8                      NaN          29.77\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-06-16 06:00:00+00:00       93.2                      0.0          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 21:00:00+00:00       78.8                      NaN          29.77\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-06-16 06:00:00+00:00       93.2                      NaN          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 21:00:00+00:00       78.8                      NaN          29.77\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2023-06-16 06:00:00+00:00       93.2                      NaN          29.74                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 21:00:00+00:00       78.8                      NaN          29.77\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table vcri sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  ALM  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-02-26 22:00:00+00:00       73.4                      0.0          29.99                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:55:00+00:00       93.2                      NaN          29.93\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-02-26 22:00:00+00:00       73.4                      0.0          29.99                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       93.2                      NaN          29.93\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-02-26 22:00:00+00:00       73.4                      NaN          29.99                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       93.2                      NaN          29.93\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-02-26 22:00:00+00:00       73.4                      NaN          29.99                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       93.2                      NaN          29.93\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table alm sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  CQT  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2020-11-03 16:00:00+00:00       62.0                   1017.8          30.06                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-20 14:47:00+00:00       60.0                   1016.1          30.01\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2020-11-03 16:00:00+00:00       62.0                   1017.8          30.06                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-20 14:00:00+00:00       60.0                   1016.1          30.01\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2020-11-03 16:00:00+00:00       62.0                   1017.8          30.06                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-20 14:00:00+00:00       60.0                   1016.1          30.01\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2020-11-03 16:00:00+00:00       62.0                   1017.8          30.06                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-05-20 14:00:00+00:00       60.0                   1016.1          30.01\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table cqt sent to DB successfully\n",
      "Query for primary key sent successfully\n",
      "Station id:  ATL  counts:  1\n",
      "get_weather_data fn\n",
      "Connected to muon database successfully\n",
      "None\n",
      "fetch_weather fn\n",
      "archive_end is null = data to today\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2019-01-17 21:00:00+00:00       47.0                   1022.7          30.19                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:52:00+00:00       73.0                   1015.3          30.01\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2019-01-17 21:00:00+00:00       47.0                   1022.7          30.19                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       73.0                   1015.3          30.01\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2019-01-17 21:00:00+00:00       47.0                   1022.7          30.19                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       73.0                   1015.3          30.01\n",
      "merged:                             temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2019-01-17 21:00:00+00:00       47.0                   1022.7          30.19                            temp_in_f  sea_l_pressure_millibar  alti_pressure\n",
      "date                                                                        \n",
      "2024-09-13 23:00:00+00:00       73.0                   1015.3          30.01\n",
      "Connected to muon database successfully\n",
      "Connected to muon database successfully\n",
      "Table atl sent to DB successfully\n",
      "Query for primary key sent successfully\n"
     ]
    }
   ],
   "source": [
    "# Format station data and upload to db\n",
    "daily_weather_to_db()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
